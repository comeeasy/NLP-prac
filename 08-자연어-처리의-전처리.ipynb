{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Tokenization<br>\n",
    "2. 단어 집합 생성<br>\n",
    "3. interger<br>\n",
    "4. encoding <br>\n",
    "5. padding<br>\n",
    "6. vectorization<br>\n",
    "\n",
    "### 1. Tokenization\n",
    "\n",
    "- 주어진 텍스트를 단어 또는 문자 단위로 자르는 것을 토큰화라고 한다.\n",
    "- spaCy\n",
    "- NLTK\n",
    "\n",
    "### spaCy 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-7-8dbec98c20da>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mspacy\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mspacy_en\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mspacy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"en\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mtokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0men_text\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[1;32mreturn\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtok\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtext\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mtok\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mspacy_en\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0men_text\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\vonenet\\lib\\site-packages\\spacy\\__init__.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(name, **overrides)\u001B[0m\n\u001B[0;32m     28\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mdepr_path\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     29\u001B[0m         \u001B[0mwarnings\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwarn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mWarnings\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mW001\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdepr_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mDeprecationWarning\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 30\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mutil\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0moverrides\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     31\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\vonenet\\lib\\site-packages\\spacy\\util.py\u001B[0m in \u001B[0;36mload_model\u001B[1;34m(name, **overrides)\u001B[0m\n\u001B[0;32m    173\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"exists\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# Path or Path-like to model data\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    174\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mload_model_from_path\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0moverrides\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 175\u001B[1;33m     \u001B[1;32mraise\u001B[0m \u001B[0mIOError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mErrors\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mE050\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    176\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    177\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mOSError\u001B[0m: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# spacy_en = spacy.load(\"en\")\n",
    "\n",
    "# def tokenize(en_text):\n",
    "#     return[tok.text for tok in spacy_en.tokenizer(en_text)]\n",
    "\n",
    "en_text = 'A Dog Run back corner near spare bedrooms'\n",
    "\n",
    "# print(tokenize(en_text))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. 단어 집합 (Vocabulary) 생성\n",
    "\n",
    "- 중복을 제거한 텍스트의 총 단어의 집합"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. 각 단어에 고유한 정수 부여\n",
    "\n",
    "- index 0, 1을 제외하고 각 단어에 인덱스를 부여"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. padding\n",
    "\n",
    "- 길이가 다른 리뷰들을 동일한 길이로 바꿔준다.\n",
    "- padding의 interger index는 1이다.\n",
    "\n",
    "# torchtext tutorial - Eng\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "- File load         : 다양한 format의 corpus를 load한다.\n",
    "- Tokenization      : 문장을 단어 단위로 분리해줍니다.\n",
    "- Vocab             : vocab을 만듬\n",
    "- Integer encoding  : 전체 corpus의 고유한 정수로 mapping\n",
    "- Word Vector       : words of Vocab의 고유한 embedding vector를 만들어준다.\n",
    "- Batching          : 훈련 샘플들의 batch를 만들어줌.\n",
    "                      padding작업도 이루어짐\n",
    "\n",
    "### 1. 훈련 데이터와 테스트 데이터로 분리하기"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import pandas as pd\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", filename=\"IMDb_Reviews.csv\")\n",
    "\n",
    "df = pd.read_csv('IMDb_Reviews.csv', encoding='latin1')\n",
    "df.head()\n",
    "\n",
    "train_df = df[:25000]\n",
    "test_df = df[25000:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "train_df.to_csv('train_data.csv', index=False)\n",
    "test_df.to_csv('test_data.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Torchtext.datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchtext.datasets import IMDB\n",
    "\n",
    "train_iter = IMDB(split='train')\n",
    "\n",
    "def tokenize(label, line):\n",
    "    return line.split()\n",
    "\n",
    "token = []\n",
    "for label, line in train_iter:\n",
    "    token += tokenize(label, line)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}